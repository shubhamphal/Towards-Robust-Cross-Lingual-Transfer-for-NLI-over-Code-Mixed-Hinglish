{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_special(s):\n",
    "   return all(i in string.punctuation for i in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = enchant.Dict(\"en_US\")\n",
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "src = \"en\"  # source language\n",
    "trg = \"hi\"  # target language\n",
    "\n",
    "model_name = f\"Helsinki-NLP/opus-mt-{src}-{trg}\"\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_hinglish_english.csv')\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df['Hinglish'][2]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # utterances = text.split(\"##\")\n",
    "    # without_speaker = []\n",
    "    # for utterance in utterances:\n",
    "    #     if(len(utterance.strip())==0):\n",
    "    #         continue\n",
    "    #     utr_split = utterance.split(\":\")\n",
    "    #     without_speaker.append(utr_split[1])\n",
    "    # result = \" \".join(without_speaker)\n",
    "    result = text.lower()\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    result = re.sub(cleanr, '', result)\n",
    "    result=re.sub(r'http\\S+', '',result)\n",
    "    result = re.sub('[0-9]+', '', result)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    result = tokenizer.tokenize(result)\n",
    "    result = \" \".join(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer https://github.com/precog-iiitd/hindi-english-code-mixing-lidf-ner for setting this up\n",
    "os.environ['HINGLISH_ROOT_DIR'] = '/home/ubuntu/teach-trans/hindi-english-code-mixing-lidf-ner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_append(temp_list):\n",
    "    roman_hin_inp = ' '.join(temp_list)\n",
    "    batch = tokenizer([roman_hin_inp], return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(**batch)\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def get_transliterated(text):\n",
    "    # Tokenised text\n",
    "    # text = [['mee'], ['too'], ['but'], ['aaj'], ['jaldi'], ['chalke'], ['dekhte'], ['hai'], [':P']]\n",
    "\n",
    "    devanagariChars   = ['\\u0900', '\\u0901', '\\u0902', '\\u0903', '\\u0904', '\\u0905', '\\u0906', '\\u0907', '\\u0908', '\\u0909', '\\u090a', '\\u090b', '\\u090c', '\\u090d', '\\u090e', '\\u090f', '\\u0910', '\\u0911', '\\u0912', '\\u0913', '\\u0914', '\\u0915', '\\u0916', '\\u0917', '\\u0918', '\\u0919', '\\u091a', '\\u091b', '\\u091c', '\\u091d', '\\u091e', '\\u091f', '\\u0920', '\\u0921', '\\u0922', '\\u0923', '\\u0924', '\\u0925', '\\u0926', '\\u0927', '\\u0928', '\\u0929', '\\u092a', '\\u092b', '\\u092c', '\\u092d', '\\u092e', '\\u092f', '\\u0930', '\\u0931', '\\u0932', '\\u0933', '\\u0934', '\\u0935', '\\u0936', '\\u0937', '\\u0938', '\\u0939', '\\u093a', '\\u093b', '\\u093c', '\\u093d', '\\u093e', '\\u093f', '\\u0940', '\\u0941', '\\u0942', '\\u0943', '\\u0944', '\\u0945', '\\u0946', '\\u0947', '\\u0948', '\\u0949', '\\u094a', '\\u094b', '\\u094c', '\\u094d', '\\u094e', '\\u094f', '\\u0950', '\\u0951', '\\u0952', '\\u0953', '\\u0954', '\\u0955', '\\u0956', '\\u0957', '\\u0958', '\\u0959', '\\u095a', '\\u095b', '\\u095c', '\\u095d', '\\u095e', '\\u095f', '\\u0960', '\\u0961', '\\u0962', '\\u0963', '\\u0964', '\\u0965', '\\u0966', '\\u0967', '\\u0968', '\\u0969', '\\u096a', '\\u096b', '\\u096c', '\\u096d', '\\u096e', '\\u096f', '\\u0970', '\\u0971', '\\u0972', '\\u0973', '\\u0974', '\\u0975', '\\u0976', '\\u0977', '\\u0978', '\\u0979', '\\u097a', '\\u097b', '\\u097c', '\\u097d', '\\u097e', '\\u097f']\n",
    "    englishCharacters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "    with open('/home/ubuntu/teach-trans/hindi-english-code-mixing-lidf-ner/transliteration/transliterationModel/englishMap', 'r') as fp:\n",
    "        EnglishMap = fp.read()\n",
    "        EnglishMap = ast.literal_eval(EnglishMap)\n",
    "\n",
    "        revEnglishMap = [0 for i in range(len(EnglishMap))]\n",
    "        for key in EnglishMap:\n",
    "            revEnglishMap[EnglishMap[key]] = key\n",
    "            \n",
    "    with open('/home/ubuntu/teach-trans/hindi-english-code-mixing-lidf-ner/transliteration/transliterationModel/hindiMap', 'r') as fp:\n",
    "        HindiMap = fp.read()\n",
    "        HindiMap = ast.literal_eval(HindiMap)\n",
    "\n",
    "        revHindiMap = [0 for i in range(len(HindiMap))]\n",
    "        for key in HindiMap:\n",
    "            revHindiMap[HindiMap[key]] = key\n",
    "\n",
    "    # print('Encoding and writing to file')\n",
    "\n",
    "    for i in range(len(text)):\n",
    "\n",
    "        cleanText = ''.join([ch for ch in text[i][0].lower() if ch in EnglishMap])\n",
    "        englishEncoding = [str(EnglishMap[ch]) for ch in cleanText]\n",
    "        text[i] += [cleanText, englishEncoding] \n",
    "\n",
    "    fp = open('/home/ubuntu/teach-trans/hindi-english-code-mixing-lidf-ner/transliteration/wordsToTransliterate.txt', 'w')\n",
    "    for token in text:\n",
    "        englishEncoding = token[2]\n",
    "        if len(englishEncoding) != 0:\n",
    "            fp.write(' '.join(englishEncoding) + '\\n')\n",
    "    fp.close()\n",
    "\n",
    "    # print('Running transliteration script')\n",
    "\n",
    "    ##################################################################\n",
    "    #\n",
    "    #\tRun Transliteration model (bash script pred.sh)\n",
    "    #\n",
    "    process = subprocess.Popen('bash /home/ubuntu/teach-trans/hindi-english-code-mixing-lidf-ner/transliteration/pred.sh ' + \n",
    "    '/home/ubuntu/teach-trans/hindi-english-code-mixing-lidf-ner/transliteration/transliterationModel/ ' +\n",
    "    '/home/ubuntu/teach-trans/hindi-english-code-mixing-lidf-ner/transliteration/wordsToTransliterate.txt ' +\n",
    "    '/home/ubuntu/teach-trans/hindi-english-code-mixing-lidf-ner/transliteration/wordsTransliterated.txt ' + \n",
    "    '/home/ubuntu/teach-trans/hindi-english-code-mixing-lidf-ner/transliteration/transliterationModel/model.ckpt-66053', shell=True, stdout=subprocess.PIPE)\n",
    "    process.wait()\n",
    "    #\n",
    "    ##################################################################\n",
    "\n",
    "    # print('Finished running transliteration script')\n",
    "\n",
    "    with open('/home/ubuntu/teach-trans/hindi-english-code-mixing-lidf-ner/transliteration/wordsTransliterated.txt', 'r') as fp:\n",
    "        transliteratedEncoding = fp.readlines()\n",
    "\n",
    "    transliteratedText  = []\n",
    "\n",
    "    for i in range(len(transliteratedEncoding)):\n",
    "        transliteratedEncoding[i] = transliteratedEncoding[i].strip('\\n').split(' ')\n",
    "        string = ''\n",
    "        for ch in transliteratedEncoding[i]:\n",
    "            if ch != '':\n",
    "                string += devanagariChars[int(ch)]\n",
    "        transliteratedText.append(string)\n",
    "        \n",
    "    mark = 0\n",
    "\n",
    "    for i in range(len(text)):\n",
    "        englishEncoding = text[i][2]\n",
    "        if len(englishEncoding) != 0:\n",
    "            text[i] += [transliteratedEncoding[mark], transliteratedText[mark]]\n",
    "            mark += 1\n",
    "        else:\n",
    "            text[i] += [[], \"\"]\n",
    "\n",
    "    # print('Original       :', ' '.join([x[0] for x in text]))\n",
    "    # print('Cleaned        :', ' '.join([x[1] for x in text]))\n",
    "    # print('Transliterated :', ' '.join([x[4] for x in text]))\n",
    "    return ' '.join([x[4] for x in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_devanagri_hindi(input):\n",
    "    li = preprocess_text(input).split()\n",
    "    ans = []\n",
    "    english_list = []\n",
    "    roman_list = []\n",
    "    for word in li:\n",
    "        if check_special(word): # Special    \n",
    "            if len(english_list) != 0:\n",
    "                if len(english_list) < 2:                \n",
    "                    english_list = [[temp] for temp in english_list]\n",
    "                    transliterated = get_transliterated(english_list)\n",
    "                    ans += transliterated.split()\n",
    "                    english_list = []      \n",
    "                else:\n",
    "                # if True:\n",
    "                    dev_hin = convert_and_append(english_list)                    \n",
    "                    ans += dev_hin.split()\n",
    "                english_list = []\n",
    "            elif len(roman_list) != 0:\n",
    "                roman_list = [[temp] for temp in roman_list]\n",
    "                transliterated = get_transliterated(roman_list)\n",
    "                ans += transliterated.split()\n",
    "                roman_list = []    \n",
    "            ans.append(word)    \n",
    "\n",
    "        elif dic.check(word): # English\n",
    "            if len(roman_list) != 0:\n",
    "                roman_list = [[temp] for temp in roman_list]\n",
    "                transliterated = get_transliterated(roman_list)\n",
    "                ans += transliterated.split()\n",
    "                roman_list = []      \n",
    "            english_list.append(word)    \n",
    "\n",
    "        else: # Roman Hindi\n",
    "            if len(english_list) != 0:\n",
    "                if len(english_list) < 2:\n",
    "                    english_list = [[temp] for temp in english_list]\n",
    "                    transliterated = get_transliterated(english_list)\n",
    "                    ans += transliterated.split()\n",
    "                    english_list = []\n",
    "                else:\n",
    "                # if True:\n",
    "                    dev_hin = convert_and_append(english_list)\n",
    "                    ans += dev_hin.split()                    \n",
    "                english_list = []\n",
    "            roman_list.append(word)\n",
    "        \n",
    "\n",
    "    if len(english_list) != 0:\n",
    "        if len(english_list) < 2:                \n",
    "            english_list = [[temp] for temp in english_list]\n",
    "            transliterated = get_transliterated(english_list)\n",
    "            ans += transliterated.split()\n",
    "            english_list = []      \n",
    "        else:\n",
    "        # if True:\n",
    "            dev_hin = convert_and_append(english_list)            \n",
    "            ans += dev_hin.split()\n",
    "        english_list = []\n",
    "    elif len(roman_list) != 0:\n",
    "        roman_list = [[temp] for temp in roman_list]\n",
    "        transliterated = get_transliterated(roman_list)\n",
    "        ans += transliterated.split()\n",
    "        roman_list = []\n",
    "\n",
    "    return ' '.join(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_devanagri_hindi(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"hi\"  # source language\n",
    "trg = \"en\"  # target language\n",
    "\n",
    "model_name = f\"Helsinki-NLP/opus-mt-{src}-{trg}\"\n",
    "hi_en_tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "hi_en_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "def devanagri_hindi_to_english(input):\n",
    "    batch = hi_en_tokenizer([input], return_tensors=\"pt\")\n",
    "    generated_ids = hi_en_model.generate(**batch)\n",
    "    return hi_en_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devanagri_hindi_to_english(convert_to_devanagri_hindi(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "refs = [[df['English'][2]]]\n",
    "\n",
    "hyps = [devanagri_hindi_to_english(convert_to_devanagri_hindi(sample))]\n",
    "\n",
    "bleu = BLEU(lowercase=True)\n",
    "\n",
    "result = bleu.corpus_score(hyps, refs)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def convert_df_hinglish_to_english(inp_df, csv_file_name = 'output.csv'):\n",
    "    devanagri_hindi_items = []\n",
    "    translated_english_items = []\n",
    "    \n",
    "    head = inp_df.head(100)\n",
    "    for index, row in tqdm(head.iterrows()):\n",
    "        hinglish = row['Hinglish']\n",
    "        devanagri_hindi = convert_to_devanagri_hindi(hinglish)\n",
    "        translated_english = devanagri_hindi_to_english(devanagri_hindi)\n",
    "        devanagri_hindi_items.append(devanagri_hindi)\n",
    "        translated_english_items.append(translated_english)\n",
    "    \n",
    "    head['Devanagri Hindi'] = devanagri_hindi_items\n",
    "    head['Translated English'] = translated_english_items\n",
    "\n",
    "    head.to_csv(csv_file_name)\n",
    "    return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_df_hinglish_to_english(df, 'output1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(csv_file_name = 'output1.csv'):\n",
    "    inp_df = pd.read_csv(csv_file_name)\n",
    "    translated_english = inp_df['Translated English']\n",
    "    target_english = inp_df['English']\n",
    "    bleu = BLEU(lowercase=True)\n",
    "\n",
    "    result = bleu.corpus_score(list(translated_english), [list(target_english)])\n",
    "    return result\n",
    "    \n",
    "calculate_bleu()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
